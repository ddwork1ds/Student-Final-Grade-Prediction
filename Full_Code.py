# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %%
df =pd.read_csv(r"D:\Machine learning\GKiMl\SP_data.csv")

# %%
df.shape

# %%
df.head()

# %%
# Xem ki·ªÉu d·ªØ li·ªáu v√† s·ªë l∆∞·ª£ng gi√° tr·ªã
df.info()

# %%
df.shape

# %%
# T·ªïng s·ªë √¥ missing trong to√†n b·ªô DataFrame
df.isna().sum().sum()

# %%
# Ki·ªÉm tra l·∫°i cho ch·∫Øc
print(f"T·ªïng s·ªë √¥ missing: {df.isna().sum().sum()}")
print(f"C√≥ missing n√†o kh√¥ng: {df.isna().any().any()}")

# %%
# Ki·ªÉm tra s·ªë l∆∞·ª£ng h√†ng tr√πng l·∫∑p
df.duplicated().sum()

# %%
df.describe()

# %%
df.describe(include="object").columns

# %%
df.describe(include=['int64', 'float64']).columns

# %%
categorical_cols= ['sex', 'Pstatus', 'paid', 'activities', 'higher', 'romantic']
for col in categorical_cols:
    print(f"Value counts for {col}: \n {df[col].value_counts()}")

# %% [markdown]
# # Ph√¢n t√≠ch ƒë∆°n bi·∫øn (Univariate Analysis): Xem x√©t t·ª´ng bi·∫øn ri√™ng l·∫ª.
# #    + Bi·∫øn s·ªë (Numerical): D√πng bi·ªÉu ƒë·ªì histogram, box plot ƒë·ªÉ xem ph√¢n ph·ªëi, t√¨m gi√° tr·ªã ngo·∫°i lai (outliers).
# #    + Bi·∫øn ph√¢n lo·∫°i (Categorical): D√πng bi·ªÉu ƒë·ªì c·ªôt (bar chart) ƒë·ªÉ xem t·∫ßn su·∫•t c·ªßa t·ª´ng danh m·ª•c

# %%
df[['age', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'health',
       'absences','G1',  'G2' ,'G3']].hist()
plt.gcf().set_size_inches(15,10)

# %%
df[['age', 'studytime', "failures", 'famrel', 'freetime', 'goout', 'health' ,"absences", 'G1', 'G2', 'G3']].skew()
# Gan 0 thi khong lech, am trai, duong phai

# %%
num_cols = ['age', 'studytime', "failures", 'famrel', 'freetime', 'goout', 'health' ,"absences", 'G1', 'G2', 'G3']

# V·∫Ω boxplot cho t·ª´ng c·ªôt
plt.figure(figsize=(15, 20))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols)//2 + 1, 2, i)  # T·ª± ƒë·ªông chia l∆∞·ªõi
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(col, fontsize=12)
plt.tight_layout()
plt.show()

# %%
for col in categorical_cols:
    value_counts = df[col].value_counts()

    # V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
    plt.figure(figsize=(8, 6))
    value_counts.plot(kind='bar')

    # Th√™m ti√™u ƒë·ªÅ v√† nh√£n
    plt.title(f'Bi·ªÉu ƒë·ªì c·ªôt cho {col}')
    plt.xlabel(col)
    plt.ylabel('S·ªë l∆∞·ª£ng')
    plt.xticks(rotation=0) # Gi·ªØ nh√£n tr·ª•c X th·∫≥ng ƒë·ª©ng

    plt.show()

# %% [markdown]
# # Ph√¢n t√≠ch ƒëa bi·∫øn
# #   +D√πng bi·ªÉu ƒë·ªì t√°n x·∫° (scatter plot) ƒë·ªÉ xem m·ªëi quan h·ªá gi·ªØa hai bi·∫øn s·ªë.
# #   + D√πng ma tr·∫≠n t∆∞∆°ng quan (correlation matrix) ƒë·ªÉ ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t∆∞∆°ng quan tuy·∫øn t√≠nh.
# #   + So s√°nh ph√¢n ph·ªëi c·ªßa m·ªôt bi·∫øn s·ªë qua c√°c danh m·ª•c kh√°c nhau b·∫±ng box plot

# %%
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".1f")
plt.title("Correlation Matrix")
plt.show()

# %%
num_features=['age', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'health',
       'absences', 'G1', 'G2']
for feature in num_features:
    sns.scatterplot(data=df, x = feature, y= 'G3') 
    plt.title(f"{feature} vs G3")
    plt.show()

# %%
for col in categorical_cols:
    sns.boxplot(data = df, x= col, y= 'G3')
    plt.title(f"G3 by {col}")
    plt.xticks(rotation=45)
    plt.show()

# %% [markdown]
# # Ti·ªÅn x·ª≠ l√≠ 
# # X√≥a b·ªõt c·ªôt, ƒê∆∞a ƒëi·ªÉm v·ªÅ thang 10,Th√™m c·ªôt G_Avg tr√°nh ƒëa c·ªông tuy·∫øn,X·ª≠ l√≠ d·ªØ li·ªáu thi·∫øu, X·ª≠ l√≠ outliers, Th√™m c·ªôt m√£ h√≥a, chu·∫©n h√≥a d·ªØ li·ªáu

# %%
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder, FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler


# %%
columns_to_drop = ['address', 'famsize', 'Medu','Fedu','Mjob', 'Fjob','reason', 'guardian','traveltime', 'schoolsup', 'famsup', 'nursery','internet', 'Dalc', 'Walc']

df = df.drop(columns=columns_to_drop)

# %%
# Chia thang ƒëi·ªÉm 20 v·ªÅ thang ƒëi·ªÉm 10
def divide_grades(X):
    X = X.copy()
    for g in ["G1", "G2", "G3"]:
        if g in X.columns and (g + "_10") not in X.columns:
            X[g + "_10"] = X[g] / 2.0
            
    print("-> ƒê√£ c·∫≠p nh·∫≠t thang ƒëi·ªÉm 20 v·ªÅ 10 cho: G1, G2, G3")
    return X

df = df.drop(columns=["G1_10", "G2_10", "G3_10"], errors="ignore")
df = divide_grades(df)
print(df[['G1', 'G1_10', 'G2', 'G2_10', 'G3', 'G3_10']].head())

# %%
df['G_Avg'] = (df['G1_10'] + df['G2_10']) / 2

# Hi·ªÉn th·ªã 5 h√†ng ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra k·∫øt qu·∫£
print(df.head())

# %%
#X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu (missing data)

# Chia c√°c c·ªôt th√†nh: c·ªôt s·ªë (numeric) v√† c·ªôt ph√¢n lo·∫°i (categorical)
num_cols = df.select_dtypes(include=['number']).columns
cat_cols = df.select_dtypes(include=['object']).columns

print("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu")
missing_before = df.isna().sum()
missing_before = missing_before[missing_before > 0]

# Ki·ªÉm tra xem c√≥ gi√° tr·ªã thi·∫øu hay kh√¥ng
if len(missing_before) == 0:
    print("‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu n√†o trong d·ªØ li·ªáu.")
else:
    print(f"C√≥ {len(missing_before)} c·ªôt c√≥ gi√° tr·ªã thi·∫øu:\n{missing_before}\n")

    #ƒêi·ªÅn gi√° tr·ªã trung b√¨nh cho c√°c c·ªôt s·ªë
    for col in num_cols:
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna(df[col].mean())
            print(f"‚úîÔ∏è ƒê√£ ƒëi·ªÅn gi√° tr·ªã trung b√¨nh cho c·ªôt s·ªë: {col}")

    #ƒêi·ªÅn gi√° tr·ªã mode (gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t) cho c√°c c·ªôt ph√¢n lo·∫°i
    for col in cat_cols:
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna(df[col].mode()[0])
            print(f"‚úîÔ∏è ƒê√£ ƒëi·ªÅn gi√° tr·ªã mode cho c·ªôt ph√¢n lo·∫°i: {col}")

    print("\nHo√†n t·∫•t x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu.")

#Ki·ªÉm tra l·∫°i sau khi x·ª≠ l√Ω
missing_after = df.isna().sum().sum()
print(f"\nT·ªïng s·ªë gi√° tr·ªã thi·∫øu sau khi x·ª≠ l√Ω: {missing_after}")


# %%
# X·ª≠ l√≠ ngo·∫°i l·ªá (outliers)
capping_cols = ['age', 'absences', 'famrel', 'studytime', 'freetime', 'G2_10']
capping_cols_present = [c for c in capping_cols if c in df.columns]  
def cap_outliers_iqr_np_all(X):
    X = X.copy()
    for col in capping_cols:
        X = cap_outliers_iqr_np(X, col)
    return X 
def cap_outliers_iqr_np(df, col):
    df = df.copy() # ƒê·ªÉ tr√°nh thay ƒë·ªïi df g·ªëc
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] > upper, upper, np.where(df[col] < lower, lower, df[col]))
    print(f"-> ƒê√£ Capping c·ªôt: {col}. Gi·ªõi h·∫°n IQR: [{lower:.2f}, {upper:.2f}]")
    return df
for col in capping_cols:
    df = cap_outliers_iqr_np(df, col)
print("‚úÖ ƒê√£ x·ª≠ l√Ω ngo·∫°i l·ªá (outliers).")

# %%
# Encode categorical variables
from sklearn.preprocessing import LabelEncoder
pre_encoding = ['school', 'sex', 'Pstatus', 'paid', 'activities', 'higher', 'romantic']
pre_encoding_present = [c for c in pre_encoding if c in df.columns] # Ki·ªÉm tra c·ªôt c√≥ trong df
encoders = {}
for col in pre_encoding_present:
    df[col] = df[col].astype(str).str.strip()  # chu·∫©n h√≥a chu·ªói
    le = LabelEncoder()
    df[col + '_encoded'] = le.fit_transform(df[col])
    mapping = {k: int(v) for v, k in enumerate(le.classes_)}
    encoders[col] = {"label_encoder": le, "mapping": mapping}
    print(f"Encoded {col} -> {col + 'encoded'} (classes: {list(le.classes_)})")
print("‚úÖ ƒê√£ m√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i.")

# %%
# Feature Scaling (chu·∫©n h√≥a d·ªØ li·ªáu)
pipeline = Pipeline([
    ('divide', FunctionTransformer(divide_grades)),
    ('cap', FunctionTransformer(cap_outliers_iqr_np_all)),
])
print("‚úÖ ƒê√£ t·∫°o pipeline chu·∫©n h√≥a d·ªØ li·ªáu.")

# %%
# Fit pipeline l√™n d·ªØ li·ªáu
fixed=pipeline.fit_transform(df)

# %%
fixed.to_csv("processed.csv", index=False)
print("‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o 'student_data_processed.csv'.")

# %%
joblib.dump(pipeline, "preprocess.pkl")
joblib.dump(encoders, "encoder.pkl")
print("‚úÖ ƒê√£ l∆∞u preprocess.pkl")
print("‚úÖ ƒê√£ l∆∞u encoder.pkl")

# %% [markdown]
# # V·∫Ω l·∫°i c√°c bi·ªÉu ƒë·ªì ƒë·ªÉ xem l·∫°i ph√¢n ph·ªëi, ki·ªÉm tra d·ªØ li·ªáu,...

# %%
# v·∫Ω l·∫°i his
df[['age', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'health',
       'absences', 'G1_10', 'G2_10', 'G3_10']].hist()
plt.gcf().set_size_inches(15,10)

# %%
# V·∫Ω l·∫°i boxplot
num_cols = ['age', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'health', 'absences', 'G1_10', 'G2_10', 'G3_10']
# V·∫Ω boxplot cho t·ª´ng c·ªôt
plt.figure(figsize=(15, 20))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols)//2 + 1, 2, i)  # T·ª± ƒë·ªông chia l∆∞·ªõi
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(col, fontsize=12)
plt.tight_layout()
plt.show()

# %%
#V·∫Ω boxplot c·ªßa c·ªôt Avg ƒë·ªÉ xem c√≥ outliers kh√¥ng
plt.figure(figsize=(6, 5))
sns.boxplot(y=df['G_Avg'], color='skyblue')

plt.title("Boxplot c·ªßa c·ªôt G_Avg", fontsize=14)
plt.ylabel("G_Avg", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# %% [markdown]
# # Training model

# %%
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


# %%
df = pd.read_csv("processed.csv")

# %%
df.head()

# %%
# --- Seed c·ªë ƒë·ªãnh (ƒë·∫£m b·∫£o t√°i l·∫≠p k·∫øt qu·∫£) ---
RANDOM_STATE = 42

# %%
# Thi·∫øt l·∫≠p Cross-Validation c·ªë ƒë·ªãnh
from sklearn.model_selection import KFold
cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

# %%
features = ['sex_encoded', 'age', 'failures', 'higher_encoded', 'absences', 'G_Avg']

target = "G3_10"

# %%
# --- Chu·∫©n b·ªã d·ªØ li·ªáu ---
X = df[features].copy()
y = df[target].copy()

# %%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)


# %%
models = {
    "Baseline(mean)": {
        "model": DummyRegressor(strategy="mean"),
        "params": {}
    },
    "LinearRegression": {
        "model": LinearRegression(),
        "params": {}
    },
    "DecisionTree": {
        "model": DecisionTreeRegressor(random_state=RANDOM_STATE),
        "params": {
            "max_depth": [3, 5, 7], 
            "min_samples_split": [5, 10], 
            "min_samples_leaf": [2, 5] 
        }
    },
    "RandomForest": {
        "model": RandomForestRegressor(random_state=RANDOM_STATE),
        "params": {
            "max_depth": [3, 7], 
            "min_samples_leaf": [5, 10, 20], 
            "n_estimators": [50, 100]
        }
    }
}

# %%
best_models=[]

# %%
models_metrics = {}

# %%
for name, config in models.items():
    print(f"Training {name}")
    
    grid = GridSearchCV(config["model"], config["params"], cv=cv, scoring="neg_mean_squared_error")
    grid.fit(X_train, y_train)

    y_pred = grid.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    best_models.append({
    "model": name,
    "best_params": grid.best_params_,
    "rmse": rmse,
    "r2": r2

   
})
    models_metrics[name] = {
        "best_params": grid.best_params_,
        "rmse": rmse,
        "r2": r2,
        "best_estimator": grid.best_estimator_
    }

# %%
best_models

# %%
results_df = pd.DataFrame(best_models)
results_df 

# %%
results_df.sort_values(by="rmse")

# %%
best_row = results_df.sort_values(by="rmse").iloc[0]

best_row

# %%
best_model_name = best_row["model"]

best_model_name


# %%
best_params = best_row['best_params']
best_params

# %%
best_model = models[best_model_name]["model"].set_params(**best_params)

# %%
best_model.fit(X_train, y_train)

# %%
joblib.dump(best_model, "best_model.pkl")

# %%
joblib.load("best_model.pkl").predict(X_test)

# %%
joblib.dump(models_metrics, "models_metrics.pkl")
print("‚úÖ ƒê√£ l∆∞u metrics c·ªßa baseline v√† candidates: models_metrics.pkl")
print("üìä Metrics ƒë√£ l∆∞u:")
for name, metrics in models_metrics.items():
    print(f"  {name}: RMSE={metrics['rmse']:.4f}, R¬≤={metrics['r2']:.4f}")


